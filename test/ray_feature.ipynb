{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 11:22:30,433\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-12-17 11:22:41,569\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-12-17 11:22:45,534\tINFO worker.py:1812 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2024-12-17 11:22:46,482\tINFO dataset.py:2631 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2024-12-17 11:22:46,485\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-12-17_11-22-41_588350_23190/logs/ray-data\n",
      "2024-12-17 11:22:46,485\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange] -> LimitOperator[limit=5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[dataset]: Run `pip install tqdm` to enable progress reporting.\n",
      "[{'id': 0}, {'id': 1}, {'id': 2}, {'id': 3}, {'id': 4}]\n",
      "Column  Type\n",
      "------  ----\n",
      "id      int64\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ds = ray.data.range(10000)\n",
    "\n",
    "print(ds.count())\n",
    "print(ds.take(5))\n",
    "print(ds.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 11:23:32,604\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-12-17_11-22-41_588350_23190/logs/ray-data\n",
      "2024-12-17 11:23:32,605\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->Write]\n",
      "2024-12-17 11:23:33,140\tINFO datasink.py:103 -- Write operation succeeded. Aggregated write results:\n",
      "\t- num_rows: 10000\n",
      "\t- size_bytes: 80000\n",
      "\n",
      "2024-12-17 11:23:33,147\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-12-17_11-22-41_588350_23190/logs/ray-data\n",
      "2024-12-17 11:23:33,147\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadCSV] -> AggregateNumRows[AggregateNumRows]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# save the dataset to a local file and load it back\n",
    "ray.data.range(10000).write_csv(\"local_dir\")\n",
    "ds = ray.data.read_csv(\"local_dir\")\n",
    "print(ds.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ray.data.range(10000)\n",
    "ds2 = ray.data.range(10000)\n",
    "ds3 = ds1.union(ds2)\n",
    "print(ds3.count())\n",
    "\n",
    "# filter the combined dataset to only the even elements\n",
    "ds3 = ds3.filter(lambda x: x % 2 ==0)\n",
    "\n",
    "# sort the filtered dataset\n",
    "ds3 = ds3.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 11:40:37,842\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2024-12-17_11-22-41_588350_23190/logs/ray-data\n",
      "2024-12-17 11:40:37,843\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadRange->MapBatches(<lambda>)] -> LimitOperator[limit=5]\n",
      "2024-12-17 11:40:37,871\tERROR streaming_executor_state.py:485 -- An exception was raised from a task of operator \"ReadRange->MapBatches(<lambda>)\". Dataset execution will now abort. To ignore this exception and continue, set DataContext.max_errored_blocks.\n"
     ]
    },
    {
     "ename": "RayTaskError(UserCodeException)",
     "evalue": "\u001b[36mray::ReadRange->MapBatches(<lambda>)()\u001b[39m (pid=23206, ip=127.0.0.1)\n  File \"/var/folders/kc/c74q_vx95dx4__wm8dtz6q540000gn/T/ipykernel_23190/845984659.py\", line 3, in <lambda>\nTypeError: unsupported operand type(s) for *: 'dict' and 'dict'\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[36mray::ReadRange->MapBatches(<lambda>)()\u001b[39m (pid=23206, ip=127.0.0.1)\n  File \"/Users/zhuoran/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_operator.py\", line 482, in _map_task\n    for b_out in map_transformer.apply_transform(iter(blocks), ctx):\n  File \"/Users/zhuoran/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 392, in __call__\n    for data in iter:\n  File \"/Users/zhuoran/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 134, in _udf_timed_iter\n    output = next(input)\n  File \"/Users/zhuoran/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 236, in __call__\n    yield from self._batch_fn(input, ctx)\n  File \"/Users/zhuoran/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 316, in transform_fn\n    res = fn(batch)\n  File \"/Users/zhuoran/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 228, in fn\n    _handle_debugger_exception(e)\n  File \"/Users/zhuoran/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 244, in _handle_debugger_exception\n    raise UserCodeException() from e\nray.exceptions.UserCodeException",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(UserCodeException)\u001b[0m           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ds \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;241m10000\u001b[39m)\u001b[38;5;241m.\u001b[39mmap_batches(\u001b[38;5;28;01mlambda\u001b[39;00m batch: np\u001b[38;5;241m.\u001b[39msquare(batch)\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m----> 4\u001b[0m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/dataset.py:2638\u001b[0m, in \u001b[0;36mDataset.take\u001b[0;34m(self, limit)\u001b[0m\n\u001b[1;32m   2635\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2637\u001b[0m limited_ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimit(limit)\n\u001b[0;32m-> 2638\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m limited_ds\u001b[38;5;241m.\u001b[39miter_rows():\n\u001b[1;32m   2639\u001b[0m     output\u001b[38;5;241m.\u001b[39mappend(row)\n\u001b[1;32m   2640\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m limit:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/iterator.py:213\u001b[0m, in \u001b[0;36mDataIterator.iter_rows.<locals>._wrapped_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_iterator\u001b[39m():\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batch_iterable:\n\u001b[1;32m    214\u001b[0m         batch \u001b[38;5;241m=\u001b[39m BlockAccessor\u001b[38;5;241m.\u001b[39mfor_block(BlockAccessor\u001b[38;5;241m.\u001b[39mbatch_to_block(batch))\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39miter_rows(public_row_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/iterator.py:154\u001b[0m, in \u001b[0;36mDataIterator.iter_batches.<locals>._create_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Iterate through the dataset from the start each time\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# _iterator_gen is called.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# This allows multiple iterations of the dataset without\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# needing to explicitly call `iter_batches()` multiple times.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m (\n\u001b[1;32m    151\u001b[0m     ref_bundles_iterator,\n\u001b[1;32m    152\u001b[0m     stats,\n\u001b[1;32m    153\u001b[0m     blocks_owned_by_consumer,\n\u001b[0;32m--> 154\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_ref_bundle_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m    157\u001b[0m     iter_batches(\n\u001b[1;32m    158\u001b[0m         ref_bundles_iterator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    170\u001b[0m )\n\u001b[1;32m    172\u001b[0m dataset_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset_tag()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/_internal/iterator/iterator_impl.py:28\u001b[0m, in \u001b[0;36mDataIteratorImpl._to_ref_bundle_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_ref_bundle_iterator\u001b[39m(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     26\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Iterator[RefBundle], Optional[DatasetStats], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m     27\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_dataset\n\u001b[0;32m---> 28\u001b[0m     ref_bundles_iterator, stats, executor \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_to_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     ds\u001b[38;5;241m.\u001b[39m_current_executor \u001b[38;5;241m=\u001b[39m executor\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ref_bundles_iterator, stats, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/exceptions.py:87\u001b[0m, in \u001b[0;36momit_traceback_stdout.<locals>.handle_trace\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m logger\u001b[38;5;241m.\u001b[39mexception(\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull stack trace:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     83\u001b[0m     exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     84\u001b[0m     extra\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhide\u001b[39m\u001b[38;5;124m\"\u001b[39m: should_hide_traceback},\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_user_code_exception:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSystemException\u001b[39;00m()\n",
      "\u001b[0;31mRayTaskError(UserCodeException)\u001b[0m: \u001b[36mray::ReadRange->MapBatches(<lambda>)()\u001b[39m (pid=23206, ip=127.0.0.1)\n  File \"/var/folders/kc/c74q_vx95dx4__wm8dtz6q540000gn/T/ipykernel_23190/845984659.py\", line 3, in <lambda>\nTypeError: unsupported operand type(s) for *: 'dict' and 'dict'\n\nThe above exception was the direct cause of the following exception:\n\n\u001b[36mray::ReadRange->MapBatches(<lambda>)()\u001b[39m (pid=23206, ip=127.0.0.1)\n  File \"/Users/zhuoran/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_operator.py\", line 482, in _map_task\n    for b_out in map_transformer.apply_transform(iter(blocks), ctx):\n  File \"/Users/zhuoran/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 392, in __call__\n    for data in iter:\n  File \"/Users/zhuoran/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 134, in _udf_timed_iter\n    output = next(input)\n  File \"/Users/zhuoran/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/_internal/execution/operators/map_transformer.py\", line 236, in __call__\n    yield from self._batch_fn(input, ctx)\n  File \"/Users/zhuoran/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 316, in transform_fn\n    res = fn(batch)\n  File \"/Users/zhuoran/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 228, in fn\n    _handle_debugger_exception(e)\n  File \"/Users/zhuoran/.pyenv/versions/3.10.14/envs/DDGRL/lib/python3.10/site-packages/ray/data/_internal/planner/plan_udf_map_op.py\", line 244, in _handle_debugger_exception\n    raise UserCodeException() from e\nray.exceptions.UserCodeException"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ds = ray.data.range(10000).map_batches(lambda batch: np.square(batch).tolist())\n",
    "ds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.map_batches(MLModel, compute='actors')\n",
    "\n",
    "cpu_intensive_preprocessing = lambda batch: batch\n",
    "gpu_intensive_inference = lambda batch: batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (ray.data.read_parquet(\"s3://my_bucket/input_data\")\n",
    "      .window(blocks_per_window=5)\n",
    "      .map(cpu_intensive_preprocessing)\n",
    "      .map_batches(gpu_intensive_inference, compute='actors', num_gpus=1)\n",
    "      .repartition(10)\n",
    "    )\n",
    "\n",
    "ds.write_parquet(\"s3://my_bucket/output_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class TrainingWorker:\n",
    "    def __init__(self, alpha: float):\n",
    "        self._model = SGDClassifier(alpha=alpha)\n",
    "\n",
    "    def train(self, train_shard: ray.data.Dataset):\n",
    "        for i, epoch in enumerate(train_shard.iter_epochs()):\n",
    "            X, Y = zip(*list(epoch.iter_rows()))\n",
    "            self._model.partial_fit(X, Y, classes=[0, 1])\n",
    "        \n",
    "        return self._model\n",
    "\n",
    "    def test(self, X_test: np.ndarray, Y_test: np.ndarray):\n",
    "        return self._model.score(X_test, Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributed part\n",
    "alpha_vals = [0.00008, 0.00009, 0.0001, 0.00011, 0.00012]\n",
    "\n",
    "print(f'Starting {len(alpha_vals)} training workers. ')\n",
    "workers = [TrainingWorker.remote(alpha) for alpha in alpha_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    *datasets.make_classification()\n",
    ")\n",
    "\n",
    "train_ds = ray.data.from_items(list(zip(X_train, Y_train)))\n",
    "shards = (train_ds.repeat(10)\n",
    "          .random_shuffle_each_window()\n",
    "          .split(len(workers), locality_hints=workers))\n",
    "\n",
    "ray.get([worker.train.remote(shard) for worker, shard in zip(workers, shards)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation results from each worker\n",
    "print(ray.get([worker.test.remote(X_test, Y_test) for worker in workers]))\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.dask import enable_dask_on_ray\n",
    "\n",
    "ray.init()\n",
    "enable_dask_on_ray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
